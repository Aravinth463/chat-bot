{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport re\nimport pickle\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:24:24.109379Z","iopub.execute_input":"2021-10-06T17:24:24.109716Z","iopub.status.idle":"2021-10-06T17:24:29.315126Z","shell.execute_reply.started":"2021-10-06T17:24:24.109635Z","shell.execute_reply":"2021-10-06T17:24:29.313935Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nINPUTS -> Encoder -> ENC OUTPUTS, THOUGHT VECTOR -> \n\nAttention Network -> Attention Weights (x ENC OUTPUTS) -> ATTENTION OUTPUT\n\nATTENTION OUTPUT, ACTUAL OUTPUT(INPUT) -> DECODER -> FINAL OUTPUT\n\"\"\"\n\n\"\"\"\nENCODER ARCHITECTURE:-\n\n\nINPUTS -> EMBEDDING -> GRU\n\"\"\"\n\n\"\"\"\nATTENTION NETWORK ARCHITECTURE\n\nENC OUTPUTS     -> ENC LAYER     -> --------       \n                                            ------> ACTIVATION -> FINAL LAYER -> ATTENTION WEIGHTS\nTHOUGHT VECTOR  -> THOUGHT LAYER -> --------\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:24:29.321828Z","iopub.execute_input":"2021-10-06T17:24:29.322649Z","iopub.status.idle":"2021-10-06T17:24:29.345299Z","shell.execute_reply.started":"2021-10-06T17:24:29.322606Z","shell.execute_reply":"2021-10-06T17:24:29.343970Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding, encoder_units, batch_size):\n        super(Encoder, self).__init__()\n        \n        self.batch_size = batch_size\n        self.enc_units = encoder_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n    \n    def call(self, inputs, hidden_state):\n        embedded_inputs = self.embedding(inputs)\n        enc_outputs, thought_vector = self.gru(embedded_inputs, initial_state=hidden_state)\n        return enc_outputs, thought_vector","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:24:29.352048Z","iopub.execute_input":"2021-10-06T17:24:29.355155Z","iopub.status.idle":"2021-10-06T17:24:29.368653Z","shell.execute_reply.started":"2021-10-06T17:24:29.355110Z","shell.execute_reply":"2021-10-06T17:24:29.367615Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        \n        self.enc_output_layer = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        self.thought_layer    = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        self.final_layer      = tf.keras.layers.Dense(1    , kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        \n    def call(self, enc_outputs, thought_vector):\n        thought_matrix = tf.expand_dims(thought_vector, 1)\n        \n        scores = self.final_layer(tf.keras.activations.tanh(self.enc_output_layer(enc_outputs) + self.thought_layer(thought_matrix)))\n        attention_weights = tf.keras.activations.softmax(scores, axis=-1)\n        \n        attention_output = attention_weights * enc_outputs # Shape (batch_size, num_outputs, output_size)\n        attention_output = tf.reduce_sum(attention_output, axis=1) # New shape (batch_size, output_size)\n        \n        return attention_output, attention_weights","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:24:29.376937Z","iopub.execute_input":"2021-10-06T17:24:29.379474Z","iopub.status.idle":"2021-10-06T17:24:29.395551Z","shell.execute_reply.started":"2021-10-06T17:24:29.379412Z","shell.execute_reply":"2021-10-06T17:24:29.394330Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding, decoder_units, batch_size):\n        super(Decoder, self).__init__()\n        \n        self.batch_size = batch_size\n        self.dec_units = decoder_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        \n        self.attention = Attention(self.dec_units)\n        self.word_output = tf.keras.layers.Dense(vocab_size, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        \n    def call(self, inputs, enc_outputs, thought_vector):\n        attention_output, attention_weights = self.attention(enc_outputs, thought_vector)\n        #Shape of attention output (batch_size, size_of_embedding)\n        \n        embedded_inputs = self.embedding(inputs) #Shape (batch_size, num_words, size_of_embedding)\n        attention_output = tf.expand_dims(attention_output, 1) #Shape of attention output (batch_size, 1, size_of_embedding)\n        concat_inputs = tf.concat([attention_output, embedded_inputs], axis=-1)\n        \n        decoder_outputs, hidden_state = self.gru(concat_inputs) #Shape (batch_size, 1, size_of_embedding)\n        decoder_outputs = tf.reshape(decoder_outputs, (-1, decoder_outputs.shape[2])) #Shape (batch_size, size_of_embedding)\n        \n        final_outputs = self.word_output(decoder_outputs)\n        return final_outputs, hidden_state, attention_weights","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:24:29.403468Z","iopub.execute_input":"2021-10-06T17:24:29.406800Z","iopub.status.idle":"2021-10-06T17:24:29.424961Z","shell.execute_reply.started":"2021-10-06T17:24:29.406755Z","shell.execute_reply":"2021-10-06T17:24:29.423675Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Train:\n    def __init__(self):\n        self.optimizer = tf.keras.optimizers.Adam()\n        self.base_loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n        \n    def loss_function(self, y_real, y_pred):\n        base_mask = tf.math.logical_not(tf.math.equal(y_real, 0))\n        base_loss = self.base_loss_function(y_real, y_pred)\n        \n        mask = tf.cast(base_mask, dtype=base_loss.dtype)\n        final_loss = mask * base_loss\n        \n        return tf.reduce_mean(final_loss)\n    \n    def train_step(self, train_data, label_data, enc_hidden, encoder, decoder, batch_size, label_tokenizer):\n        loss = 0\n        \n        with tf.GradientTape() as tape:\n            enc_outputs, thought_vector = encoder(train_data, enc_hidden)\n            dec_hidden = thought_vector\n            dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']] * batch_size, 1)\n            \n            for index in range(1, label_data.shape[1]):\n                outputs, dec_hidden, _ = decoder(dec_input, enc_outputs, dec_hidden)\n                \n                dec_input = tf.expand_dims(label_data[:, index], 1)\n                loss = loss + self.loss_function(label_data[:, index], outputs)\n        \n        word_loss = loss / int(label_data.shape[1])\n        \n        variables = encoder.trainable_variables + decoder.trainable_variables\n        gradients = tape.gradient(loss, variables)\n        self.optimizer.apply_gradients(zip(gradients, variables))\n        \n        return word_loss","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:24:29.432441Z","iopub.execute_input":"2021-10-06T17:24:29.436237Z","iopub.status.idle":"2021-10-06T17:24:29.456644Z","shell.execute_reply.started":"2021-10-06T17:24:29.436141Z","shell.execute_reply":"2021-10-06T17:24:29.455295Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n[he she it name this that these those their you]\n\ny_real = [0, 0, 0, 0, '1', 0, 0, 0, 0, 0]\n\nmath.equal() = [True, True, True, True, False, True, True, True, True, True]\nlogical not = \nmask = [1, 1, 1, 1, '0', 1, 1, 1, 1, 1]\n\n[0.001, 0.001, 0.001, 0.001, '0.9', 0.001, 0.001, 0.001, 0.001, 0.002]\n\n\n[1, 1, 1, 1, '0', 1, 1, 1, 1, 1] * [0.001, 0.001, 0.001, 0.001, '-0.1', 0.001, 0.001, 0.001, 0.001, 0.002]\n\nfinal_loss = [0.001, 0.001, 0.001, 0.001, 0, 0.001, 0.001, 0.001, 0.001, 0.002]\n\nreturn 0.0013\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:24:29.463979Z","iopub.execute_input":"2021-10-06T17:24:29.467381Z","iopub.status.idle":"2021-10-06T17:24:29.480376Z","shell.execute_reply.started":"2021-10-06T17:24:29.467339Z","shell.execute_reply":"2021-10-06T17:24:29.479146Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Data_Preprocessing:\n    def __init__(self):\n        self.temp = None\n    \n    def get_data(self, path):\n        file = open(path, 'r').read()\n        lists = [f.split('\\t') for f in file.split('\\n')]\n        \n        questions = [x[0] for x in lists]\n        answers = [x[1] for x in lists]\n        \n        return questions, answers\n    \n    def process_sentence(self, line):\n        line = line.lower().strip()\n        \n        line = re.sub(r\"([?!.,])\", r\" \\1 \", line)\n        line = re.sub(r'[\" \"]+', \" \", line)\n        line = re.sub(r\"[^a-zA-Z?!.,]+\", \" \", line)\n        line = line.strip()\n        \n        line = '<start> ' + line + ' <end>'\n        return line\n    \n    def word_to_vec(self, inputs):\n        tokenizer = Tokenizer(filters='')\n        tokenizer.fit_on_texts(inputs)\n        \n        vectors = tokenizer.texts_to_sequences(inputs)\n        vectors = pad_sequences(vectors, padding='post')\n        \n        return vectors, tokenizer","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:24:29.486413Z","iopub.execute_input":"2021-10-06T17:24:29.487139Z","iopub.status.idle":"2021-10-06T17:24:29.504409Z","shell.execute_reply.started":"2021-10-06T17:24:29.487100Z","shell.execute_reply":"2021-10-06T17:24:29.503173Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data = Data_Preprocessing()\n\nquestions, answers = data.get_data('../input/shapeai-chatbot/chatbot.txt')\n\nquestions = [data.process_sentence(str(sentence)) for sentence in questions]\nanswers = [data.process_sentence(str(sentence)) for sentence in answers]\n\ntrain_vectors, train_tokenizer = data.word_to_vec(questions)\nlabel_vectors, label_tokenizer = data.word_to_vec(answers)\n\nmax_length_train = train_vectors.shape[1]\nmax_length_label = label_vectors.shape[1]\n\nbatch_size = 64\nbuffer_size = train_vectors.shape[0]\nembedding_dim = 256\nsteps_per_epoch = buffer_size//batch_size\nunits = 1024","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:24:29.510619Z","iopub.execute_input":"2021-10-06T17:24:29.513532Z","iopub.status.idle":"2021-10-06T17:24:30.053367Z","shell.execute_reply.started":"2021-10-06T17:24:29.513492Z","shell.execute_reply":"2021-10-06T17:24:30.052216Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"vocab_train = len(train_tokenizer.word_index) + 1\nvocab_label = len(label_tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:24:30.057649Z","iopub.execute_input":"2021-10-06T17:24:30.059602Z","iopub.status.idle":"2021-10-06T17:24:30.065562Z","shell.execute_reply.started":"2021-10-06T17:24:30.059574Z","shell.execute_reply":"2021-10-06T17:24:30.064456Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((train_vectors, label_vectors))\ndataset = dataset.shuffle(buffer_size)\ndataset = dataset.batch(batch_size, drop_remainder=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:24:30.067762Z","iopub.execute_input":"2021-10-06T17:24:30.068351Z","iopub.status.idle":"2021-10-06T17:24:32.134204Z","shell.execute_reply.started":"2021-10-06T17:24:30.068309Z","shell.execute_reply":"2021-10-06T17:24:32.133173Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"encoder = Encoder(vocab_train, embedding_dim, units, batch_size)\ndecoder = Decoder(vocab_label, embedding_dim, units, batch_size)\ntrainer = Train()","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:24:32.135992Z","iopub.execute_input":"2021-10-06T17:24:32.136616Z","iopub.status.idle":"2021-10-06T17:24:32.485408Z","shell.execute_reply.started":"2021-10-06T17:24:32.136562Z","shell.execute_reply":"2021-10-06T17:24:32.484300Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 20\n\nfor epoch in range(1, EPOCHS + 1):\n    enc_hidden = tf.zeros((batch_size, units))\n    total_loss = 0\n    \n    for (batch_num, (train_data, label_data)) in enumerate(dataset.take(steps_per_epoch)):\n        batch_loss = trainer.train_step(train_data, label_data, enc_hidden, encoder, decoder, batch_size, label_tokenizer)\n        total_loss = total_loss + batch_size\n        \n    print(f\"Epoch: {epoch}, Loss: {total_loss/steps_per_epoch}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:24:32.487156Z","iopub.execute_input":"2021-10-06T17:24:32.488209Z","iopub.status.idle":"2021-10-06T17:24:41.893557Z","shell.execute_reply.started":"2021-10-06T17:24:32.488164Z","shell.execute_reply":"2021-10-06T17:24:41.891693Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}